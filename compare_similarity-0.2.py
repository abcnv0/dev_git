import sys
import tempfile
from pathlib import Path
from collections import defaultdict
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.metrics.pairwise import cosine_similarity
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
import altair as alt

# ---------------------- ë°±ì—”ë“œ í•¨ìˆ˜ ì •ì˜ ----------------------

def load_documents_from_file(file_path: str) -> list[Document]:
    """pdf, docx, txt, xlsx, ppt, pptx ë“± ë‹¤ì–‘í•œ í¬ë§·ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    suffix = Path(file_path).suffix.lower()
    docs: list[Document] = []

    if suffix == ".pdf":
        docs = PyPDFLoader(file_path).load()
    elif suffix in {".docx", ".doc"}:
        docs = Docx2txtLoader(file_path).load()
    elif suffix == ".txt":
        docs = TextLoader(file_path).load()
    elif suffix in {".xlsx", ".xls"}:
        xls = pd.read_excel(file_path, sheet_name=None)
        text = []
        for sheet_name, df in xls.items():
            text.append(f"### Sheet: {sheet_name}")
            text.append(df.astype(str).to_csv(index=False, sep=" "))
        docs = [Document(page_content="\n".join(text), metadata={"source": file_path})]
    elif suffix in {".pptx", ".ppt"}:
        try:
            from pptx import Presentation
            prs = Presentation(file_path)
            text = []
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text.append(shape.text)
            docs = [Document(page_content="\n".join(text), metadata={"source": file_path})]
        except ImportError:
            docs = []
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í¬ë§·: {suffix}")

    return docs


def compute_similarity(uploaded_path: str, db_path: str) -> list[tuple[str, float]]:
    docs = load_documents_from_file(uploaded_path)
    if not docs:
        raise ValueError(f"ì—…ë¡œë“œëœ íŒŒì¼ì— ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤: {uploaded_path}")

    query_text = docs[0].page_content
    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    query_vec = np.array(embedder.embed_documents([query_text])[0]).reshape(1, -1)

    vectordb = FAISS.load_local(db_path, embedder, allow_dangerous_deserialization=True)
    index = vectordb.index
    total = index.ntotal
    stored_vecs = index.reconstruct_n(0, total)
    stored_matrix = np.array(stored_vecs)
    meta_docs = list(vectordb.docstore._dict.values())

    sims = cosine_similarity(query_vec, stored_matrix)[0]
    file_scores = defaultdict(float)
    for md, sim in zip(meta_docs, sims):
        src = md.metadata.get("source", "")
        fname = Path(src).name if src else "ì•Œ ìˆ˜ ì—†ìŒ"
        file_scores[fname] = max(file_scores[fname], sim)

    results = sorted(file_scores.items(), key=lambda x: -x[1])
    return [(fn, sc * 100) for fn, sc in results]


def rebuild_db(source_folder: str, db_path: str):
    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    docs: list[Document] = []
    for file in Path(source_folder).iterdir():
        try:
            docs.extend(load_documents_from_file(str(file)))
        except Exception:
            continue
    if not docs:
        raise ValueError(f"ì§€ì›ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {source_folder}")
    vectordb = FAISS.from_documents(docs, embedder)
    vectordb.save_local(db_path)

# ---------------------- Streamlit UI ì„¤ì • ----------------------
st.set_page_config(page_title="document Analyzer", layout="wide", initial_sidebar_state="expanded")

# CSS ì»¤ìŠ¤í„°ë§ˆì´ì§•
st.markdown(
    """
    <style>
    /* ì‚¬ì´ë“œë°” ì „ì²´ ê¸€ì 200% */
    section[data-testid="stSidebar"] * { font-size: 200% !important; }
    .css-1x92ibr { max-width: 90%; margin: auto; }
    
    /* ì‚¬ì´ë“œë°” ë‚´ë¶€ì˜ ëª¨ë“  ë ˆì´ë¸” í…ìŠ¤íŠ¸ë¥¼ 200%ë¡œ í‚¤ìš°ê³  ë³¼ë“œ ì²˜ë¦¬ */
    section[data-testid="stSidebar"] label,
    section[data-testid="stSidebar"] .stButton>button {
        font-size: 400% !important;
        font-weight: bold !important;
    }

    /* ë©”ì¸ ì˜ì—­ì˜ header (h2) í¬ê¸°ë„ ë” í¬ê²Œ ì¡°ì • */
    .css-1lsmgbg h2, /* í´ë˜ìŠ¤ëª…ì€ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ, ê°œë°œì ë„êµ¬ë¡œ í™•ì¸í•˜ì„¸ìš” */
    .css-1lsmgbg .stHeader {
        font-size: 4.5rem !important;
    }
    
    </style>
    """, unsafe_allow_html=True
)

st.title("ğŸ“„ document Analyzer")
sidebar, main = st.columns([1, 4])

with sidebar:
    st.header("Index Folder")
    src_folder = st.text_input("ì†ŒìŠ¤ í´ë”", value="source_documents")
    db_folder = st.text_input("FAISS ì¸ë±ìŠ¤ í´ë”", value="db")
    if st.button("ì¸ë±ìŠ¤ ì¬ë¹Œë“œ"):
        try:
            rebuild_db(src_folder, db_folder)
            st.success(f"ì¸ë±ìŠ¤ ì¬ë¹Œë“œ ì™„ë£Œ: {src_folder} â†’ {db_folder}")
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ì¬ë¹Œë“œ ì‹¤íŒ¨: {e}")

with main:
    st.header("File Uploader")
    uploaded_file = st.file_uploader(
        "íŒŒì¼ì„ ë“œë˜ê·¸ ì•¤ ë“œë¡­í•˜ì„¸ìš”",
        type=["pdf", "doc", "docx", "txt", "xlsx", "ppt", "pptx"],
        label_visibility="visible"
    )

    if uploaded_file:
        suffix = Path(uploaded_file.name).suffix.lower()
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        tmp.write(uploaded_file.getbuffer()); tmp.flush()

        st.info(f"ì—…ë¡œë“œë¨: **{uploaded_file.name}**")
        st.info(f"ì‚¬ìš© ì¤‘ì¸ DB: **{db_folder}**")

        try:
            results = compute_similarity(tmp.name, db_folder)
            df = pd.DataFrame(results, columns=["íŒŒì¼ëª…", "ìœ ì‚¬ë„ (%)"]).sort_values(by="ìœ ì‚¬ë„ (%)", ascending=False)

            st.subheader("ëª¨ë“  íŒŒì¼ì˜ ìœ ì‚¬ë„ (%)")
            st.dataframe(df, use_container_width=True)

            st.subheader("ìœ ì‚¬ë„ ê°€ë¡œ ë§‰ëŒ€ ì°¨íŠ¸")
            chart = alt.Chart(df).mark_bar(cornerRadius=5).encode(
                y=alt.Y('íŒŒì¼ëª…:N', sort='-x', title=None, axis=alt.Axis(labelFontSize=14)),
                x=alt.X('ìœ ì‚¬ë„ (%):Q', title='ìœ ì‚¬ë„ (%)'),
                color=alt.value('#4B8BBE')
            ).properties(height=400)
            st.altair_chart(chart, use_container_width=True)

            df['row'] = ' '

            st.subheader("ìœ ì‚¬ë„ íˆíŠ¸ë§µ")
            heatmap = alt.Chart(df).mark_rect().encode(
                x=alt.X('íŒŒì¼ëª…:N', sort='-row', axis=alt.Axis(labelAngle=-45, labelFontSize=12)),
                y=alt.Y('row:O', axis=None),
                color=alt.Color('ìœ ì‚¬ë„ (%):Q', scale=alt.Scale(scheme='viridis'), title='ìœ ì‚¬ë„ (%)')
            ).properties(height=80)
            st.altair_chart(heatmap, use_container_width=True)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")


